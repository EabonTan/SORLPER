#!/usr/bin/env python3

# -*- coding: utf-8 -*-



import rospy

import numpy as np

import random

import torch

import torch.nn as nn

import torch.nn.functional as F

import pickle

from geometry_msgs.msg import Twist

from sensor_msgs.msg import LaserScan

from nav_msgs.msg import Odometry

import math

from gazebo_msgs.srv import SetModelState

from gazebo_msgs.msg import ModelState

from geometry_msgs.msg import Pose, Quaternion





def get_observations(agent_pos, friend_pos, enemy_pos):  # 最近静态障碍物

    enemy_pos_r = [0, 0]  # 初始化 enemy_pos_r

    enemy_pos_r[0] = enemy_pos[1] + agent_pos[0]

    enemy_pos_r[1] = enemy_pos[0] + agent_pos[1]



    enemy_dis = np.linalg.norm(np.array(enemy_pos_r) - np.array(agent_pos)) - 0.4  # - 0.3   0.4keyi  



    fri_dis = np.linalg.norm(friend_pos - agent_pos)

    fri_angle = math.atan2((friend_pos - agent_pos)[1], (friend_pos - agent_pos)[0])

    print(fri_dis, enemy_dis, fri_angle)



    return fri_dis, enemy_dis, fri_angle





class GridWorld:

    def __init__(self, grid_size, start, goal):

        rospy.init_node('dn1_robot', anonymous=True)

        # Publisher and Subscriber

        self.cmd_pub = rospy.Publisher('/cmd_vel', Twist, queue_size=10)

        self.laser_sub = rospy.Subscriber('/scan', LaserScan, self.callback_laser_scan, queue_size=1)

        self.odom_sub = rospy.Subscriber('/odom', Odometry, self.callback_odometry, queue_size=1)  # Now referencing a defined method

        self.grid_size = grid_size

        self.start = start

        self.goal = goal

        self.obstacles = [0, 0]

        self.reset()

        self.reset_robot_position()

        self.d_em = 0.1  # 碰撞结束

        self.d_fri = 0.5  # 到达结束

        self.cur_em_dis = 0

        self.cur_fri_dis = 0



    def obstacles(self):  

        # 获取当前机器人的位姿

        odom_data = rospy.wait_for_message('/odom', Odometry)

        agent_pos = self.agent_position

        # 障碍物信息

        laser_scan_data = rospy.wait_for_message('/scan', LaserScan)

        # DN input

        enemy_angle, enemy_pos = self.callback_laser_scan(laser_scan_data)

        enemy_pos_r = [0, 0]

        # obstacle zuobiao bianhuan

        enemy_pos_r[0] = enemy_pos[1] + agent_pos[0]

        enemy_pos_r[1] = enemy_pos[0] + agent_pos[1]



        return enemy_pos_r



    def reset_robot_position(self):

        reset_service = rospy.ServiceProxy('/gazebo/set_model_state', SetModelState)

        model_state = ModelState()

        model_state.model_name = 'autolabor_pro1'

        model_state.pose.position.x = self.start[0]

        model_state.pose.position.y = self.start[1]

        model_state.pose.orientation.w = 1.0  # Default orientation

        reset_service(model_state)



    def reset(self):

        self.position = self.start

        return self.position



    def callback_laser_scan(self, scan_data):

        # 将雷达数据转换为笛卡尔坐标

        laser_ranges = scan_data.ranges

        angle_min = scan_data.angle_min

        angle_increment = scan_data.angle_increment

        laser_points = []

        for i, distance in enumerate(laser_ranges):

            if distance < scan_data.range_max:

                angle = angle_min + i * angle_increment

                x = distance * math.cos(angle)

                y = distance * math.sin(angle)

                laser_points.append((x, y))



        # 找到最近的障碍物

        d_e, d_index = min(laser_ranges), laser_ranges.index(min(laser_ranges))

        enemy_angle = angle_min + d_index * angle_increment  

        enemy = (math.cos(enemy_angle) * d_e, math.sin(enemy_angle) * d_e)

        if enemy_angle > -math.pi/2 and enemy_angle < math.pi:

            enemy_angle =  enemy_angle + math.pi/2

        elif enemy_angle >= -math.pi and enemy_angle < math.pi/2:

            enemy_angle =  enemy_angle + 5*math.pi/2



        # print("Obstacle angle:{}".format(enemy_angle))



        return enemy_angle, enemy

   

    # The missing callback function for the Odometry data

    def callback_odometry(self, odom_data):

        # Process odometry data to update the robot's position and current angle

        self.agent_position = (odom_data.pose.pose.position.x, odom_data.pose.pose.position.y)

        quaternion = (

            odom_data.pose.pose.orientation.x,

            odom_data.pose.pose.orientation.y,

            odom_data.pose.pose.orientation.z,

            odom_data.pose.pose.orientation.w

        )

        euler = self.quaternion_to_euler(quaternion)

        self.current_angle = euler[2]  # 获取当前朝向 (in radians)



    def quaternion_to_euler(self, quat):

        # Convert quaternion to Euler angles

        x, y, z, w = quat

        t0 = +2.0 * (w * x + y * z)

        t1 = +1.0 - 2.0 * (x * x + y * y)

        roll_x = math.atan2(t0, t1)



        t2 = +2.0 * (w * y - z * x)

        t2 = +1.0 if t2 > +1.0 else t2

        t2 = -1.0 if t2 < -1.0 else t2

        pitch_y = math.asin(t2)



        t3 = +2.0 * (w * z + x * y)

        t4 = +1.0 - 2.0 * (y * y + z * z)

        theta_z = math.atan2(t3, t4)



        return roll_x, pitch_y, theta_z  # 返回滚转、俯仰、偏航角

        self.current_angle = euler[2]  # 获取当前朝向 (in radians)



    def quaternion_to_euler(self, quat):

        # Convert quaternion to Euler angles

        x, y, z, w = quat

        t0 = +2.0 * (w * x + y * z)

        t1 = +1.0 - 2.0 * (x * x + y * y)

        roll_x = math.atan2(t0, t1)



        t2 = +2.0 * (w * y - z * x)

        t2 = +1.0 if t2 > +1.0 else t2

        t2 = -1.0 if t2 < -1.0 else t2

        pitch_y = math.asin(t2)



        t3 = +2.0 * (w * z + x * y)

        t4 = +1.0 - 2.0 * (y * y + z * z)

        theta_z = math.atan2(t3, t4)



        return roll_x, pitch_y, theta_z  # 返回滚转、俯仰、偏航角



    def get_current_angle(self):

        return self.current_angle  # 返回当前角度



    def move_robot(self, odom_data, action):

        cmd_msg = Twist()        

        target_angle = action * (360 / 36)  # Convert action to angle        

        target_angle_rad = math.radians(target_angle)  # Convert angle to rad

        if target_angle_rad < math.pi:

            target_angle_rad = target_angle_rad + self.current_angle

        else:

            target_angle_rad = target_angle_rad - 2 * math.pi + self.current_angle

        tolerance = math.radians(0.1)  # 定义一个小的阈值来确定何时停止旋转 

        current_angle_rad = self.current_angle

        print("the current_angle_rad", current_angle_rad)

        print("the target_angle_rad", target_angle_rad)



        # 旋转到目标角度

        while True:  

            self.get_current_angle()

            current_angle_rad = self.current_angle

            angle_diff_rad = (target_angle_rad - current_angle_rad + math.pi) % (2 * math.pi) - math.pi  # 计算角度差异

            # angle_diff_rad = target_angle_rad - current_angle_rad

            if abs(angle_diff_rad) < tolerance:  

                rospy.sleep(0.2)  # Check again after a brief pause to ensure it's stable

                if abs(angle_diff_rad) < tolerance:  # Double-check the angle difference

                    break  # Target angle reached, stop rotation  



            # 根据角度差异设置旋转速度  

            cmd_msg.angular.z = 0.4 if angle_diff_rad > 0 else -0.4  

            self.cmd_pub.publish(cmd_msg)  

            rospy.sleep(0.1)  # 等待一小段时间



        # 停止旋转

        cmd_msg.angular.z = 0.0  

        self.cmd_pub.publish(cmd_msg)

        rospy.sleep(1)  # 确保停顿



        # 前进

        cmd_msg.linear.x = 0.2  # 前进速度

        self.cmd_pub.publish(cmd_msg)

        rospy.sleep(1)  # 向前移动



        # 停止移动

        cmd_msg.linear.x = 0.0

        self.cmd_pub.publish(cmd_msg)

        print("the diff of target_angle_rad and current_angle_rad", angle_diff_rad)





    def step(self, action, agent_trajectory):

        

        # 获取当前机器人的位姿

        odom_data = rospy.wait_for_message('/odom', Odometry)

        agent_pos = self.agent_position

        # 获取目标点和障碍物信息

        friend_pos = self.goal  # 目标点

        laser_scan_data = rospy.wait_for_message('/scan', LaserScan)

        # DN input

        enemy_angle, enemy_pos = self.callback_laser_scan(laser_scan_data)

        cur_em_angle = enemy_angle

        cur_fri_dis, cur_em_dis, cur_fri_angle = get_observations(agent_pos, friend_pos, enemy_pos)

        self.cur_fri_dis = cur_fri_dis

        self.cur_em_dis = cur_em_dis

        arrive = 0



        # 根据决策控制机器人

        self.move_robot(odom_data, action)  # 

        # 获取下一时刻机器人的位姿

        odom_data_next = rospy.wait_for_message('/odom', Odometry)

        # 获取目标点和障碍物信息

        laser_scan_data_next = rospy.wait_for_message('/scan', LaserScan)

        # DN input

        enemy_angle_next, enemy_pos_next = self.callback_laser_scan(laser_scan_data_next)

        new_position = self.agent_position



        fri_dis, em_dis, fri_angle = get_observations(new_position, friend_pos, enemy_pos_next)

        fri_angle = fri_angle - self.current_angle

        

        if self.is_valid_position(new_position):

            self.position = tuple(new_position)

        else:

            self.position = self.position



        length_trajectory = int(len(agent_trajectory))

        # 结束条件

        if (em_dis < self.d_em) or (length_trajectory > 100):

            done = True

            arrive = 0

            # r = -1

        elif fri_dis < self.d_fri:   # d_fri = 10

            done = True

            arrive = 1

            # r = 10

        else:

            done = False

            # r = 0

        r = self.get_reward(fri_dis, em_dis, length_trajectory)



        return np.array(self.position), r, done, arrive



    def is_valid_position(self, position):

        if (0 <= position[0] < self.grid_size[0]) and (0 <= position[1] < self.grid_size[1]):

            return tuple(position) not in self.obstacles

        return False



    def get_reward(self, fri_dis, em_dis, length_trajectory):

        if (em_dis < self.d_em) or (length_trajectory > 1000):

            r1 = -50

        else:

            r1 = 0



        if fri_dis < self.d_fri:  

            r2 = 100  

        else:

            r2 = 0



        if fri_dis < self.cur_fri_dis:  

            r3 = 1  

        else:

            r3 = -1



        if self.cur_em_dis < 0.25:

            if em_dis > self.cur_em_dis:

                r4 = 1

            else:

                r4 = -0.5                

        else:

            r4 = 0



        reward = r1 + r2 + r3 + r4



        return reward





class DQN(nn.Module):

    def __init__(self, state_size, action_size, device):

        super(DQN, self).__init__()

        self.fc = nn.Sequential(

            nn.Linear(state_size, 128),

            nn.ReLU(),

            nn.Linear(128, 128),

            nn.ReLU(),

            nn.Linear(128, action_size)

        )



    def forward(self, x):

        return self.fc(x)





class DQNTester:

    def __init__(self, model_path, env):

        self.weighting_decay = 10

        self.env = env

        self.device = torch.device("cpu")

        self.q_net = DQN(6, 36, device=torch.device("cpu")).to(self.device)

        self.q_net.load_state_dict(torch.load(model_path))  # Load the trained model

        self.q_net.eval()  # Set the network to evaluation mode



        file_path = "env1-model/data_QValues4.pkl"  

        file_path2 = "env1-model/data_units4.pkl"  



        with open(file_path, "rb") as f:

            self.QValues = pickle.load(f)



        with open(file_path2, "rb") as f:

            self.units = pickle.load(f)



    def GetBestUnit(self, state):



        best_unit = np.argmin(np.sum((self.units['w'] - state) ** 2, axis=-1), axis=0)



        return best_unit



    def GetWeighting(self, best_unit, state):



        diff = np.sum(np.square(self.units['w'][best_unit, :] - state))

        w = np.exp(-diff / self.weighting_decay)



        return w



    def GetQValues(self, state, q_graph_values):



        best_unit = self.GetBestUnit(state)

        som_action_values = self.QValues[best_unit, :]

        w = self.GetWeighting(best_unit, state)

        q_values = (w * som_action_values) + ((1 - w) * q_graph_values)



        return q_values



    def run_test(self):

        agent_state = self.env.reset()

         # 获取当前机器人的位姿

        odom_data = rospy.wait_for_message('/odom', Odometry)

        agent_pos = np.array(self.env.agent_position)



        # 获取目标点和障碍物信息

        friend_pos = self.env.goal  # 目标点

        laser_scan_data = rospy.wait_for_message('/scan', LaserScan)

        # DQN input

        enemy_angle, enemy_pos = self.env.callback_laser_scan(laser_scan_data)

        em_angle = enemy_angle

        fri_dis, em_dis, fri_angle = get_observations(agent_pos, friend_pos, enemy_pos)

        fri_angle = fri_angle - self.env.current_angle

        arrive = 0



        state = [np.array([np.cos(fri_angle), np.sin(fri_angle), np.cos(em_angle), np.sin(em_angle),

                          fri_dis / (fri_dis + em_dis), em_dis / (fri_dis + em_dis)])]

        done = False

        total_reward = 0    

        trajectory = agent_pos.reshape(1, 2)   

        step = 0

        while not done:

            state_temp = torch.tensor(state, dtype=torch.float).to(self.device)

            with torch.no_grad():

                q_graph_values = self.q_net(state_temp).cpu().detach().numpy()  # 添加cpu()

            q_values = self.GetQValues(state, q_graph_values)

            action = np.argmax(q_values)

            next_agent_state, reward, done, info = self.env.step(action, trajectory)  # Take action in the environment

            

            odom_data = rospy.wait_for_message('/odom', Odometry)

            agent_pos = np.array(self.env.agent_position)

            # 获取目标点和障碍物信息

            friend_pos = self.env.goal  

            laser_scan_data = rospy.wait_for_message('/scan', LaserScan)

            # DQN input

            enemy_angle, enemy_pos = self.env.callback_laser_scan(laser_scan_data)

            em_angle = enemy_angle

            fri_dis, em_dis, fri_angle = get_observations(agent_pos, friend_pos, enemy_pos)

            fri_angle = fri_angle - self.env.current_angle

            print("fri_angle:{}".format(fri_angle))

            print("em_angle:{}".format(em_angle))

            arrive = 0

            next_state = [np.array([np.cos(fri_angle), np.sin(fri_angle), np.cos(em_angle), np.sin(em_angle),

                            fri_dis / (fri_dis + em_dis), em_dis / (fri_dis + em_dis)])]



            total_reward += reward

            trajectory = np.r_[trajectory, agent_pos.reshape(1, 2)]  # Record agent's position

            state = next_state  # Update state

            if len(trajectory) > 200:

                done = True

            step += 1



        print('Total reward: {}'.format(total_reward))

        print('Total step: {}'.format(step))

        return trajectory





if __name__ == "__main__":

    try:         

        # Define grid size, start, goal

        map_width = 10

        map_height = 10

        grid_size = (map_width, map_height)

        start = np.array([0, 0])

        goal = np.array([4, 4])  



        # Create grid world environment

        env = GridWorld(grid_size, start, goal)

        print(torch.cuda.is_available())         

        model_path = "env1-model/Q_net4.pth"  # Specify the path to your trained model

        tester = DQNTester(model_path, env)  # Initialize the tester

        trajectory = tester.run_test()  # Run the test



    except rospy.ROSInterruptException:

        pass

     



